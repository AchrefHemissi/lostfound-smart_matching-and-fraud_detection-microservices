
# ğŸ§  lostfound-ai-service

A modular FastAPI microservice for generating and searching image/text embeddings using OpenAIâ€™s CLIP model and a vector store (Qdrant). Designed to be scalable and production-ready.

---

## ğŸ“ Project Structure

```text
lostfound-ai-service/
â”‚
â”œâ”€â”€ app/                     # Main application code
â”‚   â”œâ”€â”€ main.py             # FastAPI app entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                # API layer (HTTP routes and dependencies)
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚   â”‚       â””â”€â”€ embeddings.py   # API routes for text/image embedding
â”‚   â”‚   â””â”€â”€ dependencies.py         # Shared dependencies (model loaders, DB, etc.)
â”‚   â”‚
â”‚   â”œâ”€â”€ core/               # Core settings and configuration
â”‚   â”‚   â”œâ”€â”€ config.py       # Loads environment variables and app settings
â”‚   â”‚   â””â”€â”€ logger.py       # App-wide logging configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ services/           # Core business logic
â”‚   â”‚   â”œâ”€â”€ clip_service.py     # CLIP model loading and embedding
â”‚   â”‚   â””â”€â”€ vector_service.py   # Vector database (Qdrant/FAISS) operations
â”‚   â”‚
â”‚   â”œâ”€â”€ models/             # Pydantic request/response models
â”‚   â”‚   â””â”€â”€ embedding_request.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/              # Utility functions
â”‚       â””â”€â”€ image_utils.py  # Image preprocessing for CLIP
â”‚
â”œâ”€â”€ tests/                  # Unit and integration tests
â”‚   â”œâ”€â”€ test_clip_service.py
â”‚   â””â”€â”€ test_vector_service.py
â”‚
â”œâ”€â”€ test_images/            # Sample images for testing
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project overview and instructions
```

---

## ğŸ” Component Descriptions

- **app/main.py**  
  Initializes and runs the FastAPI app.

- **app/api/**  
  Defines all HTTP-accessible routes and shared dependencies.

- **app/api/v1/endpoints/embeddings.py**  
  POST endpoints to embed images or text.

- **app/api/dependencies.py**  
  Reusable FastAPI dependencies like model loaders, DB sessions.

- **app/core/**  
  Environment configuration and logging setup.

- **app/services/**  
  Main logic for interacting with CLIP and the vector store.

- **app/models/**  
  Pydantic schemas for request validation and response shaping.

- **app/utils/**  
  Utility helpers like image pre-processing.

- **tests/**  
  Automated tests to ensure service correctness.

---

## ğŸ§° Tech Stack & Tools

| Tool               | Description |
|--------------------|-------------|
| **FastAPI**        | ğŸš€ High-performance Python web framework used to expose the API. |
| **Uvicorn**        | âš¡ ASGI server to run FastAPI apps. |
| **PyTorch**        | ğŸ§  Deep learning framework used by CLIP. |
| **Torchvision**    | ğŸ–¼ï¸ Image utilities and pretrained models required by CLIP. |
| **FAISS (faiss-cpu)** | ğŸ” Fast vector similarity search (for local/dev use). |
| **Pillow**         | ğŸ–¼ï¸ Image loading and processing. |
| **python-dotenv**  | ğŸ” Loads secrets and environment variables from `.env`. |
| **CLIP (OpenAI)**  | ğŸ“·ğŸ“ Creates shared embeddings for images and text. |
| **Qdrant**         | ğŸ“¦ Vector database for storing and searching embeddings (alternative to FAISS). |

---

## â–¶ï¸ Running the Project

### 1. Create and activate a virtual environment

```bash
# On macOS/Linux:
python -m venv venv
source venv/bin/activate

# On Windows:
python -m venv venv
venv\Scripts\activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Start the app

```bash
uvicorn app.main:app --reload
```

---
