Hereâ€™s a full breakdown of the project architecture:

---

## ğŸ“ Project Structure

```text
lostfound-ai-service/
â”‚
â”œâ”€â”€ app/                     # Main application code
â”‚   â”œâ”€â”€ main.py             # FastAPI app entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                # API layer (HTTP routes and dependencies)
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚   â”‚       â””â”€â”€ embeddings.py   # API routes for text/image embedding
â”‚   â”‚   â””â”€â”€ dependencies.py         # Shared dependencies (model loaders, DB, etc.)
â”‚   â”‚
â”‚   â”œâ”€â”€ core/               # Core settings and configuration
â”‚   â”‚   â”œâ”€â”€ config.py       # Loads environment variables and app settings
â”‚   â”‚   â””â”€â”€ logger.py       # App-wide logging configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ services/           # Core business logic
â”‚   â”‚   â”œâ”€â”€ clip_service.py     # CLIP model loading and embedding
â”‚   â”‚   â””â”€â”€ vector_service.py   # Vector database (Qdrant/FAISS) operations
â”‚   â”‚
â”‚   â”œâ”€â”€ models/             # Pydantic request/response models
â”‚   â”‚   â””â”€â”€ embedding_request.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/              # Utility functions
â”‚       â””â”€â”€ image_utils.py  # Image preprocessing for CLIP
â”‚
â”œâ”€â”€ tests/                  # Unit and integration tests
â”‚   â”œâ”€â”€ test_clip_service.py
â”‚   â””â”€â”€ test_vector_service.py
â”‚
â”œâ”€â”€ test_images/            # Sample images for testing
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project overview and instructions
```

---

## ğŸ” Component Descriptions

- **app/main.py**  
  Initializes and runs the FastAPI app.

- **app/api/**  
  Defines all HTTP-accessible routes and shared dependencies.

- **app/api/v1/endpoints/embeddings.py**  
  POST endpoints to embed images or text.

- **app/api/dependencies.py**  
  Reusable FastAPI dependencies like model loaders, DB sessions.

- **app/core/**  
  Environment configuration and logging setup.

- **app/services/**  
  Main logic for interacting with CLIP and the vector store.

- **app/models/**  
  Pydantic schemas for request validation and response shaping.

- **app/utils/**  
  Utility helpers like image pre-processing.

- tests/  
  Contains all unit and integration tests to ensure the code works correctly and reliably.

Those are the main tools used on this project :

fastapi
ğŸš€ What: A modern web framework for building APIs with Python 3.7+
ğŸ“Œ Why: You'll use FastAPI to expose your AI model as an API endpoint (e.g., receive a post and return similar items).
âœ… Chosen because it's fast, async-ready, easy to document, and great for microservices.

uvicorn
ğŸš€ What: A lightning-fast ASGI server to run FastAPI apps
ğŸ“Œ Why: FastAPI needs an ASGI server to run. Uvicorn handles incoming requests and serves your app.
âœ… Chosen for its performance and compatibility with FastAPI.

torch
ğŸ§  What: PyTorch, a deep learning framework
ğŸ“Œ Why: CLIP (the model you'll use for embeddings) is built on PyTorch.
âœ… Chosen because CLIP depends on it, and itâ€™s widely used for deep learning.

torchvision
ğŸ–¼ï¸ What: PyTorchâ€™s library for image transformations and pretrained models
ğŸ“Œ Why: Required by CLIP and helpful for processing images before passing them into the model.
âœ… Chosen because it complements PyTorch perfectly for image tasks.

faiss-cpu
ğŸ” What: Facebook AI Similarity Search â€” a library for fast vector search
ğŸ“Œ Why: You need to find similar vectors (image+text embeddings). FAISS does this super fast, especially on large datasets.
âœ… Chosen for performance in similarity search. Youâ€™re using the CPU version for local/dev simplicity.

pillow
ğŸ–¼ï¸ What: A Python image processing library
ğŸ“Œ Why: You'll need to load and manipulate images before feeding them to CLIP.
âœ… Chosen because it's lightweight, easy to use, and commonly used for image I/O.

python-dotenv
ğŸ” What: Loads environment variables from a .env file
ğŸ“Œ Why: Youâ€™ll likely want to keep secrets (e.g., API keys, config options) outside your code.
âœ… Chosen because it's simple and helps you follow 12-factor app practices.

git+https://github.com/openai/CLIP.git
ğŸ§  What: The actual CLIP model (Contrastive Languageâ€“Image Pre-training) by OpenAI
ğŸ“Œ Why: This is the model you're using to create shared embeddings from text & images.
âœ… Chosen because CLIP is state-of-the-art for aligning images and text in the same embedding space.

qdrant-client
ğŸ§  What: Python SDK for Qdrant â€” a vector database
ğŸ“Œ Why: You might store embeddings in a vector DB instead of just FAISS (better for production, filtering, metadata, etc).
âœ… Chosen because Qdrant is modern, open-source, fast, and offers filtering + metadata support.

How to run the project

python -m venv venv
venv\Scripts\activate

````

### 2. Install dependencies

```bash
pip install -r requirements.txt
````
