Hereâ€™s a full breakdown of the project architecture:

ğŸ“¦ Project: lostfound-ai-service  
This service allows users to generate and search image/text embeddings using OpenAIâ€™s CLIP model and a vector store which is Qdrant. Itâ€™s designed to be modular, scalable, and production-ready.

ğŸ“ Root Structure:

lostfound-ai-service/  
â”‚  
â”œâ”€â”€ app/ # Main application package  
â”‚ â”œâ”€â”€ main.py # Entry point that creates and runs the FastAPI app  
â”‚ â”‚  
â”‚ â”œâ”€â”€ api/ # API layer - HTTP routes & dependencies  
â”‚ â”‚ â”œâ”€â”€ v1/ # Versioned API (e.g. /v1/embedding)  
â”‚ â”‚ â”‚ â””â”€â”€ endpoints/ # All versioned route handlers  
â”‚ â”‚ â”‚ â””â”€â”€ embeddings.py # Defines routes for text/image embedding  
â”‚ â”‚ â””â”€â”€ dependencies.py # Shared dependency injection (e.g., model loader, DB session)  
â”‚ â”‚  
â”‚ â”œâ”€â”€ core/ # Core configuration utilities  
â”‚ â”‚ â”œâ”€â”€ config.py # Loads environment variables and app settings  
â”‚ â”‚ â””â”€â”€ logger.py # Standardized logging configuration  
â”‚ â”‚  
â”‚ â”œâ”€â”€ services/ # Business logic and services  
â”‚ â”‚ â”œâ”€â”€ clip_service.py # Handles CLIP model loading and embedding generation  
â”‚ â”‚ â””â”€â”€ vector_service.py # Handles vector indexing and similarity search (Qdrant/FAISS)  
â”‚ â”‚  
â”‚ â”œâ”€â”€ models/ # Pydantic schemas for validation and serialization  
â”‚ â”‚ â””â”€â”€ embedding_request.py # Request and response models for embedding API  
â”‚ â”‚  
â”‚ â””â”€â”€ utils/ # Utility modules and helpers  
â”‚ â””â”€â”€ image_utils.py # Image pre-processing and formatting for the CLIP model  
â”‚  
â”œâ”€â”€ tests/ # Unit and integration test suite  
â”‚ â”œâ”€â”€ test_clip_service.py # Tests for embedding logic  
â”‚ â””â”€â”€ test_vector_service.py # Tests for vector search logic  
â”‚  
â”œâ”€â”€ test_images/ # Sample images for testing embedding/search  
â”œâ”€â”€ .env # Environment variables (e.g., vector DB URL, settings)  
â”œâ”€â”€ requirements.txt # List of required Python packages  
â””â”€â”€ README.md # Project documentation and setup guide

ğŸ” Breakdown of Core Roles:

- app/main.py  
  Initializes and runs the FastAPI app. Wires routes, middleware, and config.

- app/api/  
  Defines all HTTP-accessible functionality. This is the "controller" layer that communicates between the outside world and internal logic.

- app/api/v1/endpoints/embeddings.py  
  Defines POST routes to process user requests (like uploading an image or text to embed it).

- app/api/dependencies.py  
  Provides reusable FastAPI dependencies â€” like a loaded model, DB session, or shared configs.

- app/core/  
  Everything related to core app behavior â€” config loading and logging. This makes it easy to centralize things like env parsing and system diagnostics.

- app/services/  
  Encapsulates the logic for interacting with models and vector databases. Think of these as the brains of the app â€” where embeddings are generated and searched.

- app/models/  
  Defines the data shape that your API expects or returns. Keeps I/O clean and strongly validated.

- app/utils/  
  Utility helpers â€” usually stateless functions for tasks like image resizing, normalization, etc.

- tests/  
  Contains all unit and integration tests to ensure the code works correctly and reliably.

Those are the main tools used on this project :

fastapi
ğŸš€ What: A modern web framework for building APIs with Python 3.7+
ğŸ“Œ Why: You'll use FastAPI to expose your AI model as an API endpoint (e.g., receive a post and return similar items).
âœ… Chosen because it's fast, async-ready, easy to document, and great for microservices.

uvicorn
ğŸš€ What: A lightning-fast ASGI server to run FastAPI apps
ğŸ“Œ Why: FastAPI needs an ASGI server to run. Uvicorn handles incoming requests and serves your app.
âœ… Chosen for its performance and compatibility with FastAPI.

torch
ğŸ§  What: PyTorch, a deep learning framework
ğŸ“Œ Why: CLIP (the model you'll use for embeddings) is built on PyTorch.
âœ… Chosen because CLIP depends on it, and itâ€™s widely used for deep learning.

torchvision
ğŸ–¼ï¸ What: PyTorchâ€™s library for image transformations and pretrained models
ğŸ“Œ Why: Required by CLIP and helpful for processing images before passing them into the model.
âœ… Chosen because it complements PyTorch perfectly for image tasks.

faiss-cpu
ğŸ” What: Facebook AI Similarity Search â€” a library for fast vector search
ğŸ“Œ Why: You need to find similar vectors (image+text embeddings). FAISS does this super fast, especially on large datasets.
âœ… Chosen for performance in similarity search. Youâ€™re using the CPU version for local/dev simplicity.

pillow
ğŸ–¼ï¸ What: A Python image processing library
ğŸ“Œ Why: You'll need to load and manipulate images before feeding them to CLIP.
âœ… Chosen because it's lightweight, easy to use, and commonly used for image I/O.

python-dotenv
ğŸ” What: Loads environment variables from a .env file
ğŸ“Œ Why: Youâ€™ll likely want to keep secrets (e.g., API keys, config options) outside your code.
âœ… Chosen because it's simple and helps you follow 12-factor app practices.

git+https://github.com/openai/CLIP.git
ğŸ§  What: The actual CLIP model (Contrastive Languageâ€“Image Pre-training) by OpenAI
ğŸ“Œ Why: This is the model you're using to create shared embeddings from text & images.
âœ… Chosen because CLIP is state-of-the-art for aligning images and text in the same embedding space.

qdrant-client
ğŸ§  What: Python SDK for Qdrant â€” a vector database
ğŸ“Œ Why: You might store embeddings in a vector DB instead of just FAISS (better for production, filtering, metadata, etc).
âœ… Chosen because Qdrant is modern, open-source, fast, and offers filtering + metadata support.

How to run the project

python -m venv venv
source venv/bin/activate # on Mac/Linux

# or

venv\Scripts\activate # on Windows

pip install -r requirements.txt
